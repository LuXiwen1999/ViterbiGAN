{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% 使用GAN生成信道转移向量，完成训练\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n发送长度为K的导频符号序列，其中发送符号x[i]从发送集S中等概率随机选取，接收端接受y[i]\\n训练数据即T[i]=(S[i],y[i])\\n收集多个i时刻的信道输入状态向量与信道输出（K个），从而得到训练数据集T=[T1,T2,...Ti]\\n'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "发送长度为K的导频符号序列，其中发送符号x[i]从发送集S中等概率随机选取，接收端接受y[i]\n",
    "训练数据即T[i]=(S[i],y[i])\n",
    "收集多个i时刻的信道输入状态向量与信道输出（K个），从而得到训练数据集T=[T1,T2,...Ti]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step1. 开始构建数据生成器，产生S[i]与y[i]的数据对\n",
    "根据后面的看法，似乎所有的观测值有一个特定的集合，y[i]只会在特定集合内获取\n",
    "最终的网络输出相当于成为了一个分类问题\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NumbersDataset(Dataset):\n",
    "    def __init__(self, low, high):\n",
    "        self.samples = list(range(low, high))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transmitter = self.samples[idx]\n",
    "        receiver = transmitter + 1\n",
    "        return transmitter, receiver"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "1280"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_batch_data(batch_size):\n",
    "    global start_idx, data\n",
    "    if start_idx + batch_size >= data_size:\n",
    "        start_idx = 0\n",
    "        data = np.random.binomial(1, 0.5, [data_size, block_length, 1])\n",
    "    batch_x = data[start_idx:start_idx + batch_size]\n",
    "    start_idx += batch_size\n",
    "    return batch_x\n",
    "\n",
    "block_length = 4\n",
    "N_training = int(1e6)\n",
    "data = np.random.binomial(1, 0.5, [N_training, block_length, 1])\n",
    "data_size = len(data)\n",
    "start_idx = 0\n",
    "batch_x = generate_batch_data(320)\n",
    "batch_x.size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "(5, 6)\n",
      "[tensor([49, 26, 15, 28, 10,  3, 32, 13,  4, 36]), tensor([50, 27, 16, 29, 11,  4, 33, 14,  5, 37])]\n"
     ]
    }
   ],
   "source": [
    "dataset = NumbersDataset(0,50)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "print(len(dataset))\n",
    "print(dataset[5])\n",
    "print(next(iter(dataloader)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 构建关于数据生成器的测试\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use a fake function to replace the viterbiNet\n",
    "g() -> p -> q(1,10)\n",
    "\"\"\"\n",
    "def fakeFunction(p):\n",
    "    return p[1:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step2. 开始构建两个全连接神经网络D和G，使参数满足标准高斯分布\n",
    "所述的生成器G是一个全连接神经网络\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(G, self).__init__()\n",
    "        self.FC1 = nn.Sequential(nn.Linear(4,256), nn.LeakyReLU())\n",
    "        self.FC2 = nn.Sequential(nn.Linear(256,512), nn.LeakyReLU())\n",
    "        self.FC3 = nn.Sequential(nn.Linear(512,1024), nn.LeakyReLU())\n",
    "        self.FC4 = nn.Linear(1024,p)\n",
    "    def forward(self, s):\n",
    "#         隐含层层数为Ng\n",
    "        s = self.FC1(s)\n",
    "        s = self.FC2(s)\n",
    "        s = self.FC3(s)\n",
    "        s = self.FC4(s)\n",
    "        return nn.Softmax(s)\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.FC1a = nn.Sequential(nn.Linear(10,256), nn.LeakyReLU())\n",
    "        self.FC1b = nn.Sequential(nn.Linear(4,256), nn.LeakyReLU())\n",
    "        self.FC2 = nn.Sequential(nn.Linear(512,512), nn.LeakyReLU())\n",
    "        self.FC3 = nn.Sequential(nn.Linear(512,256), nn.LeakyReLU())\n",
    "        self.FC4 = nn.Sequential(nn.Linear(256,1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, q, s):\n",
    "        s1a = self.FC1a(q)\n",
    "        s1b = self.FC1b(s)\n",
    "        s = torch.cat((s1a,s1b),2)\n",
    "        # print(s.size())\n",
    "        s = self.FC2(s)\n",
    "        s = self.FC3(s)\n",
    "        s = self.FC4(s)\n",
    "        return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G(\n",
      "  (FC1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC2): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC3): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC4): Linear(in_features=1024, out_features=4, bias=True)\n",
      ")\n",
      "FC1.0.weight : torch.Size([256, 4])\n",
      "FC1.0.bias : torch.Size([256])\n",
      "FC2.0.weight : torch.Size([512, 256])\n",
      "FC2.0.bias : torch.Size([512])\n",
      "FC3.0.weight : torch.Size([1024, 512])\n",
      "FC3.0.bias : torch.Size([1024])\n",
      "FC4.weight : torch.Size([4, 1024])\n",
      "FC4.bias : torch.Size([4])\n",
      "tensor([[1., 1., 1., 1.]])\n",
      "torch.Size([1, 4])\n",
      "Softmax(dim=tensor([[-0.0350,  0.0836, -0.0285,  0.0247]], grad_fn=<AddmmBackward>))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "在这里要\n",
    "1. 输出网络的结构(print,torch.summary两种)\n",
    "2. 查看网络的现有每层参数\n",
    "3. 构造数据，喂入网络，查看网络的输出\n",
    "'''\n",
    "\n",
    "g = G(4)\n",
    "print(g)\n",
    "for name,parameters in g.named_parameters():\n",
    "    print(name,':',parameters.size())\n",
    "#     print(parameters)\n",
    "\n",
    "a = torch.ones(1,4)\n",
    "print(a)\n",
    "print(g.forward(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 查看网络结构\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D(\n",
      "  (FC1a): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC1b): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC3): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC4): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "tensor([[1., 1., 1., 1.]]) tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 256])\n",
      "tensor([[0.5011]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "d = D()\n",
    "print(d)\n",
    "b = torch.ones(1,10)\n",
    "s = torch.ones(1,4)\n",
    "print(s,b)\n",
    "print(d.forward(b,s))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 对鉴别器D进行同样的测试\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 4])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           1,280\n",
      "         LeakyReLU-2               [-1, 1, 256]               0\n",
      "            Linear-3               [-1, 1, 512]         131,584\n",
      "         LeakyReLU-4               [-1, 1, 512]               0\n",
      "            Linear-5              [-1, 1, 1024]         525,312\n",
      "         LeakyReLU-6              [-1, 1, 1024]               0\n",
      "            Linear-7                 [-1, 1, 4]           4,100\n",
      "================================================================\n",
      "Total params: 662,276\n",
      "Trainable params: 662,276\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 2.53\n",
      "Estimated Total Size (MB): 2.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g.cuda()\n",
    "summary(g,(1,4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 10])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 1, 256])\n",
      "torch.Size([2, 1, 256])\n",
      "torch.Size([2, 1, 512])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "         LeakyReLU-2               [-1, 1, 256]               0\n",
      "            Linear-3               [-1, 1, 256]           1,280\n",
      "         LeakyReLU-4               [-1, 1, 256]               0\n",
      "            Linear-5               [-1, 1, 512]         262,656\n",
      "         LeakyReLU-6               [-1, 1, 512]               0\n",
      "            Linear-7               [-1, 1, 256]         131,328\n",
      "         LeakyReLU-8               [-1, 1, 256]               0\n",
      "            Linear-9                 [-1, 1, 1]             257\n",
      "          Sigmoid-10                 [-1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 398,337\n",
      "Trainable params: 398,337\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 1.52\n",
      "Estimated Total Size (MB): 1.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "d = D()\n",
    "d.cuda()\n",
    "summary(d,[(1,10),(1,4)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 8])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(1,4)\n",
    "b = torch.ones(1,4)\n",
    "c = torch.cat((a,b),1)\n",
    "\n",
    "c.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "D(\n  (FC1a): Sequential(\n    (0): Linear(in_features=10, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC1b): Sequential(\n    (0): Linear(in_features=4, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC2): Sequential(\n    (0): Linear(in_features=512, out_features=512, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC3): Sequential(\n    (0): Linear(in_features=512, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC4): Sequential(\n    (0): Linear(in_features=256, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step3. 对两个网络进行初始化，按照高斯分布初始化参数结构\n",
    "'''\n",
    "\n",
    "# 初始化函数接受一个初始化过的网络作为参数输入，将其参数重新初始化为高斯分布\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('FC') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "NetG = G(4)\n",
    "NetD = D()\n",
    "\n",
    "# 【标记】关于此处参数是否高斯分布生效需要进一步做验证\n",
    "NetG.apply(weights_init)\n",
    "NetD.apply(weights_init)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1.0.weight : Parameter containing:\n",
      "tensor([[ 0.3531,  0.1266,  0.0548, -0.3789],\n",
      "        [ 0.2874, -0.0368, -0.3237,  0.4367],\n",
      "        [ 0.3923, -0.3982, -0.3290,  0.1248],\n",
      "        ...,\n",
      "        [-0.0855,  0.0312, -0.4437,  0.3043],\n",
      "        [-0.2566, -0.3790,  0.3619, -0.2201],\n",
      "        [-0.3218, -0.4271, -0.3628,  0.4894]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "FC1.0.bias : Parameter containing:\n",
      "tensor([-0.0770,  0.3751,  0.0234,  0.3199,  0.2731,  0.0814,  0.3278,  0.1989,\n",
      "        -0.0365,  0.1530,  0.3693,  0.3298, -0.4652, -0.3019,  0.2739, -0.1169,\n",
      "        -0.2305, -0.1288,  0.3569, -0.3330,  0.4296, -0.0127,  0.0105,  0.1142,\n",
      "         0.1382, -0.2168, -0.4757,  0.2725, -0.2356, -0.1166, -0.2571, -0.1681,\n",
      "        -0.4064, -0.1575,  0.3892,  0.4138,  0.3721, -0.0012, -0.2221,  0.2309,\n",
      "         0.0093, -0.1243,  0.2976,  0.3366,  0.1136, -0.3105,  0.0144, -0.1863,\n",
      "        -0.3968,  0.0702, -0.0020,  0.3529,  0.2240, -0.0490, -0.3077,  0.1001,\n",
      "         0.4068, -0.4215,  0.1955,  0.1706, -0.0300,  0.3692,  0.2744, -0.1987,\n",
      "         0.4280, -0.2843,  0.1694,  0.2438, -0.1677, -0.2514,  0.1135,  0.4084,\n",
      "        -0.1341,  0.4091,  0.3950, -0.1178, -0.2265,  0.2102,  0.4342, -0.1663,\n",
      "         0.4098,  0.2783,  0.3066, -0.0422, -0.2693, -0.4080, -0.0499,  0.4943,\n",
      "         0.0336,  0.3710,  0.2206,  0.3887, -0.1126,  0.3707, -0.3773,  0.3243,\n",
      "        -0.1824,  0.4032, -0.2056,  0.1330, -0.4509,  0.4228, -0.2530, -0.0586,\n",
      "        -0.0321, -0.0207, -0.3491, -0.3566, -0.0547, -0.0702, -0.2349, -0.3613,\n",
      "        -0.3750, -0.2320, -0.3521,  0.3572, -0.2797, -0.1784,  0.2997, -0.3336,\n",
      "        -0.1344, -0.4776,  0.3973,  0.3533,  0.3753,  0.4543, -0.1221,  0.4120,\n",
      "        -0.2428, -0.4480,  0.4690,  0.1019,  0.3249, -0.2233, -0.1208, -0.3809,\n",
      "        -0.1920, -0.3768,  0.3233,  0.2961,  0.3397, -0.0953,  0.2448,  0.1033,\n",
      "         0.3831, -0.4407, -0.1068,  0.2439, -0.1379, -0.0430, -0.0037, -0.2317,\n",
      "         0.4254, -0.0368, -0.2138, -0.3587,  0.4397, -0.3588, -0.0137, -0.0631,\n",
      "        -0.4445,  0.1678,  0.4930, -0.1259, -0.3595,  0.0598, -0.4080, -0.1815,\n",
      "         0.3955,  0.2338, -0.2843,  0.3935,  0.4868, -0.1103, -0.0525,  0.1515,\n",
      "         0.4949, -0.1333,  0.2698, -0.4280, -0.4229, -0.1929,  0.4775,  0.1568,\n",
      "        -0.2648,  0.4119,  0.2566, -0.3055, -0.4120,  0.4740, -0.4218, -0.2824,\n",
      "        -0.3508, -0.3369,  0.3034, -0.2653, -0.0440,  0.4405,  0.0926,  0.4484,\n",
      "        -0.1509,  0.1244, -0.0788,  0.1416, -0.3210, -0.1462,  0.0871, -0.3308,\n",
      "        -0.4337, -0.3528,  0.1600, -0.4850,  0.3521,  0.1839, -0.2261,  0.0751,\n",
      "        -0.1567,  0.3651, -0.3485, -0.0441, -0.4484, -0.1969, -0.2946, -0.4028,\n",
      "         0.0707,  0.2962, -0.0506,  0.0671,  0.0917, -0.3017, -0.0056,  0.1503,\n",
      "         0.3719,  0.2102, -0.3704,  0.4704,  0.2367, -0.0394, -0.1739, -0.4156,\n",
      "         0.3000, -0.3729, -0.3371, -0.1407, -0.4409,  0.4964, -0.2860,  0.0994,\n",
      "        -0.1243, -0.1767, -0.0086, -0.2259,  0.1747, -0.4925,  0.2715,  0.0643],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "FC2.0.weight : Parameter containing:\n",
      "tensor([[-1.5265e-02, -1.0582e-02,  4.6631e-02,  ...,  2.2737e-02,\n",
      "          5.6127e-02,  3.6458e-02],\n",
      "        [ 1.6671e-02,  2.4472e-02, -4.6572e-02,  ..., -5.1663e-02,\n",
      "          5.6706e-02,  3.1493e-02],\n",
      "        [-7.6961e-04,  8.5176e-03, -2.2295e-02,  ..., -2.0605e-02,\n",
      "         -2.4135e-02, -2.0404e-02],\n",
      "        ...,\n",
      "        [ 1.2194e-02,  2.6024e-03,  1.3349e-03,  ..., -1.1352e-02,\n",
      "          2.3681e-02, -3.0147e-02],\n",
      "        [-2.6109e-02,  3.5523e-02,  3.2842e-02,  ..., -5.4639e-03,\n",
      "         -1.5127e-02,  4.9761e-02],\n",
      "        [ 4.0504e-02, -6.2488e-05,  2.8077e-02,  ..., -3.6208e-03,\n",
      "         -2.0703e-04, -3.2815e-02]], device='cuda:0', requires_grad=True)\n",
      "FC2.0.bias : Parameter containing:\n",
      "tensor([ 0.0109,  0.0125, -0.0259, -0.0530, -0.0213, -0.0558,  0.0351,  0.0477,\n",
      "         0.0577,  0.0563, -0.0105, -0.0605, -0.0497, -0.0123, -0.0187,  0.0117,\n",
      "        -0.0339, -0.0204,  0.0331,  0.0581, -0.0616,  0.0407,  0.0144,  0.0281,\n",
      "        -0.0072,  0.0161,  0.0122, -0.0473, -0.0355, -0.0140, -0.0353, -0.0147,\n",
      "        -0.0497, -0.0311, -0.0603, -0.0208, -0.0053, -0.0604, -0.0313, -0.0452,\n",
      "        -0.0084,  0.0053,  0.0171,  0.0137, -0.0515, -0.0364, -0.0332,  0.0453,\n",
      "        -0.0460, -0.0245, -0.0310, -0.0025, -0.0099,  0.0510, -0.0218, -0.0314,\n",
      "        -0.0277,  0.0239, -0.0406,  0.0116, -0.0115, -0.0579, -0.0240, -0.0213,\n",
      "        -0.0138,  0.0493, -0.0235, -0.0331, -0.0447, -0.0367, -0.0190,  0.0536,\n",
      "        -0.0432, -0.0335,  0.0458, -0.0362, -0.0551,  0.0572,  0.0142, -0.0294,\n",
      "         0.0080,  0.0017, -0.0045,  0.0511,  0.0404, -0.0352, -0.0339, -0.0240,\n",
      "        -0.0608,  0.0529, -0.0164,  0.0401,  0.0264, -0.0208, -0.0239, -0.0336,\n",
      "        -0.0489,  0.0225,  0.0496, -0.0026,  0.0095, -0.0507, -0.0462,  0.0370,\n",
      "         0.0283, -0.0417,  0.0353,  0.0036,  0.0175,  0.0556,  0.0338, -0.0403,\n",
      "         0.0467, -0.0087, -0.0163,  0.0256,  0.0519,  0.0161,  0.0598,  0.0283,\n",
      "         0.0535,  0.0280,  0.0384, -0.0429, -0.0163, -0.0334,  0.0599,  0.0363,\n",
      "         0.0102,  0.0004, -0.0394,  0.0374, -0.0467, -0.0331, -0.0175,  0.0128,\n",
      "         0.0005, -0.0285,  0.0550, -0.0120, -0.0346,  0.0579,  0.0256, -0.0507,\n",
      "        -0.0211,  0.0032,  0.0087, -0.0249,  0.0366,  0.0622,  0.0617,  0.0600,\n",
      "         0.0382,  0.0408, -0.0402,  0.0602, -0.0565,  0.0202,  0.0428, -0.0058,\n",
      "        -0.0229,  0.0420,  0.0217, -0.0456, -0.0127, -0.0060, -0.0247, -0.0135,\n",
      "        -0.0038,  0.0547,  0.0346,  0.0507, -0.0118, -0.0359,  0.0497, -0.0376,\n",
      "         0.0276, -0.0266, -0.0407,  0.0448,  0.0030,  0.0229,  0.0593, -0.0571,\n",
      "         0.0347,  0.0332,  0.0163,  0.0278, -0.0111,  0.0557, -0.0412,  0.0221,\n",
      "        -0.0174,  0.0597,  0.0451, -0.0264, -0.0199,  0.0182,  0.0224, -0.0535,\n",
      "        -0.0535,  0.0508,  0.0323, -0.0367,  0.0246,  0.0392, -0.0168, -0.0079,\n",
      "        -0.0496,  0.0270, -0.0265,  0.0206,  0.0289, -0.0496,  0.0386, -0.0007,\n",
      "        -0.0104, -0.0074,  0.0205,  0.0341,  0.0405, -0.0164,  0.0213,  0.0611,\n",
      "        -0.0150,  0.0624, -0.0222,  0.0219,  0.0245,  0.0004, -0.0450,  0.0092,\n",
      "        -0.0328,  0.0074, -0.0049,  0.0257, -0.0014, -0.0261, -0.0177, -0.0354,\n",
      "         0.0562, -0.0547,  0.0562,  0.0296,  0.0371, -0.0572, -0.0590,  0.0258,\n",
      "         0.0520, -0.0327, -0.0598,  0.0225, -0.0391,  0.0256, -0.0220, -0.0501,\n",
      "         0.0077,  0.0201,  0.0428,  0.0363, -0.0201,  0.0145, -0.0122,  0.0248,\n",
      "         0.0074,  0.0349,  0.0610, -0.0151, -0.0454,  0.0232,  0.0115, -0.0281,\n",
      "         0.0245,  0.0305,  0.0085, -0.0600, -0.0281, -0.0540, -0.0042,  0.0339,\n",
      "        -0.0504, -0.0529, -0.0457, -0.0191,  0.0009,  0.0241, -0.0114, -0.0300,\n",
      "        -0.0163,  0.0067,  0.0571, -0.0110, -0.0239, -0.0576, -0.0406, -0.0148,\n",
      "         0.0392,  0.0128, -0.0292, -0.0388,  0.0300, -0.0068, -0.0481,  0.0305,\n",
      "        -0.0550,  0.0243,  0.0100, -0.0019,  0.0273, -0.0258, -0.0023, -0.0598,\n",
      "         0.0170, -0.0352,  0.0105,  0.0446,  0.0181, -0.0257, -0.0380, -0.0177,\n",
      "         0.0463, -0.0044, -0.0517,  0.0271,  0.0244,  0.0501,  0.0365,  0.0220,\n",
      "        -0.0484, -0.0115, -0.0231,  0.0245,  0.0480,  0.0325, -0.0154,  0.0591,\n",
      "         0.0093,  0.0372,  0.0519, -0.0377, -0.0601,  0.0336,  0.0145, -0.0443,\n",
      "        -0.0141,  0.0400, -0.0594, -0.0238,  0.0141, -0.0532,  0.0354, -0.0010,\n",
      "         0.0077, -0.0016, -0.0028, -0.0089, -0.0126,  0.0181,  0.0513,  0.0597,\n",
      "        -0.0361,  0.0119, -0.0492, -0.0047,  0.0429, -0.0529,  0.0082,  0.0403,\n",
      "         0.0254,  0.0255,  0.0044,  0.0390, -0.0173,  0.0537, -0.0224,  0.0496,\n",
      "         0.0263,  0.0176, -0.0489,  0.0606, -0.0254, -0.0494, -0.0257,  0.0501,\n",
      "        -0.0518,  0.0255,  0.0163, -0.0165, -0.0516, -0.0368, -0.0563,  0.0038,\n",
      "        -0.0273,  0.0233,  0.0028, -0.0501, -0.0623,  0.0110, -0.0466,  0.0088,\n",
      "         0.0462,  0.0237,  0.0147, -0.0177, -0.0298, -0.0096, -0.0020, -0.0566,\n",
      "        -0.0308, -0.0365,  0.0323, -0.0308,  0.0179,  0.0476,  0.0217, -0.0355,\n",
      "        -0.0575, -0.0569, -0.0466,  0.0144,  0.0160,  0.0569, -0.0496,  0.0039,\n",
      "         0.0336,  0.0168, -0.0509,  0.0306,  0.0278,  0.0019, -0.0233, -0.0109,\n",
      "         0.0287,  0.0120,  0.0624,  0.0031,  0.0332,  0.0339, -0.0224, -0.0167,\n",
      "         0.0427, -0.0310, -0.0234,  0.0361, -0.0424,  0.0430, -0.0400,  0.0401,\n",
      "        -0.0246,  0.0504,  0.0539,  0.0328, -0.0059,  0.0348,  0.0267, -0.0408,\n",
      "         0.0526, -0.0372,  0.0488,  0.0271,  0.0035, -0.0252,  0.0054,  0.0401,\n",
      "         0.0107, -0.0272,  0.0406, -0.0060, -0.0183,  0.0336,  0.0273, -0.0084,\n",
      "         0.0487, -0.0430, -0.0076,  0.0004,  0.0367,  0.0080, -0.0040,  0.0086,\n",
      "         0.0324,  0.0065, -0.0461, -0.0601,  0.0464,  0.0531,  0.0125, -0.0430,\n",
      "        -0.0030,  0.0502,  0.0409,  0.0116, -0.0372, -0.0183, -0.0108,  0.0488,\n",
      "         0.0110,  0.0293,  0.0285, -0.0528,  0.0581, -0.0404,  0.0353,  0.0254,\n",
      "         0.0337, -0.0589,  0.0445, -0.0536,  0.0240, -0.0473, -0.0357, -0.0508],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "FC3.0.weight : Parameter containing:\n",
      "tensor([[-0.0350,  0.0268,  0.0229,  ..., -0.0368,  0.0428,  0.0164],\n",
      "        [ 0.0439,  0.0027,  0.0374,  ..., -0.0027,  0.0005, -0.0258],\n",
      "        [ 0.0051,  0.0225,  0.0145,  ...,  0.0421, -0.0014, -0.0326],\n",
      "        ...,\n",
      "        [-0.0003,  0.0218, -0.0400,  ...,  0.0028,  0.0010, -0.0217],\n",
      "        [ 0.0273,  0.0147, -0.0018,  ..., -0.0086, -0.0012, -0.0048],\n",
      "        [-0.0122, -0.0376,  0.0199,  ...,  0.0121, -0.0126, -0.0364]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "FC3.0.bias : Parameter containing:\n",
      "tensor([ 0.0058, -0.0273, -0.0374,  ..., -0.0028, -0.0437,  0.0168],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "FC4.weight : Parameter containing:\n",
      "tensor([[ 2.3015e-02,  8.9999e-03, -1.1232e-02,  ...,  6.7003e-05,\n",
      "          1.3575e-02, -2.1900e-02],\n",
      "        [-2.2383e-02, -1.8386e-03, -2.5379e-02,  ..., -1.7240e-02,\n",
      "         -3.7152e-03, -1.2272e-02],\n",
      "        [-2.8647e-02,  2.5811e-02,  2.1988e-02,  ...,  2.9539e-02,\n",
      "          7.8821e-03,  3.0046e-02],\n",
      "        [ 2.4138e-03, -4.0107e-03,  2.1047e-02,  ..., -9.0826e-03,\n",
      "         -2.8545e-02,  5.7838e-03]], device='cuda:0', requires_grad=True)\n",
      "FC4.bias : Parameter containing:\n",
      "tensor([ 0.0071, -0.0007, -0.0238,  0.0147], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in g.named_parameters():\n",
    "    print(name,':',parameters)\n",
    "#     print(parameters)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 查看网络参数\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "'''\n",
    "Step4. 分别定义两个网络的损失函数和优化方法，进行训练\n",
    "'''\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "# 这里产生一个固定的噪声，每轮学习完成后都使用固定噪声来进行输出\n",
    "# fixed_noise = torch.randn(25, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "optimizerD = torch.optim.Adam(NetD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(NetG.parameters(), lr=lr, betas=(beta1, 0.999))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Step5. 开始进行网络训练\n",
    "首先对鉴别器进行训练，然后训练生成器\n",
    "'''\n",
    "# Training Loop\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "ngpu = 1\n",
    "nz = 100\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "dataset = NumbersDataset(0, 50)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        NetD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = NetD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = NetG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = NetD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        NetG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = NetD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}