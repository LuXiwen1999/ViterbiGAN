{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% 使用GAN生成信道转移向量，完成训练\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n发送长度为K的导频符号序列，其中发送符号x[i]从发送集S中等概率随机选取，接收端接受y[i]\\n训练数据即T[i]=(S[i],y[i])\\n收集多个i时刻的信道输入状态向量与信道输出（K个），从而得到训练数据集T=[T1,T2,...Ti]\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "发送长度为K的导频符号序列，其中发送符号x[i]从发送集S中等概率随机选取，接收端接受y[i]\n",
    "训练数据即T[i]=(S[i],y[i])\n",
    "收集多个i时刻的信道输入状态向量与信道输出（K个），从而得到训练数据集T=[T1,T2,...Ti]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step1. 开始构建数据生成器，产生S[i]与y[i]的数据对\n",
    "根据后面的看法，似乎所有的观测值有一个特定的集合，y[i]只会在特定集合内获取\n",
    "最终的网络输出相当于成为了一个分类问题\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NumbersDataset(Dataset):\n",
    "    def __init__(self, block_length):\n",
    "        data_size = pow(2, block_length)\n",
    "        self.samples = np.random.binomial(1, 0.5, [data_size, 1, block_length])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transmitter = self.samples[idx]\n",
    "        # transmitter = torch.from_numpy(transmitter)\n",
    "        transmitter = torch.FloatTensor(transmitter)\n",
    "        # receiver, true q\n",
    "        receiver = torch.ones(1,10)\n",
    "        return transmitter, receiver"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "(tensor([[1., 1., 1., 1.]]), tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]))\n",
      "[tensor([[[0., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.]]]), tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])]\n"
     ]
    }
   ],
   "source": [
    "dataset = NumbersDataset(4)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "print(len(dataset))\n",
    "print(dataset[0])\n",
    "print(next(iter(dataloader)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use a fake function to replace the viterbiNet\n",
    "g() -> p -> q(1,10)\n",
    "\"\"\"\n",
    "def fakeFunction(batch_size, p):\n",
    "    return torch.ones(batch_size, 1,10).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.ones(1,15)\n",
    "fakeFunction(p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step2. 开始构建两个全连接神经网络D和G，使参数满足标准高斯分布\n",
    "所述的生成器G是一个全连接神经网络\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(G, self).__init__()\n",
    "        self.FC1 = nn.Sequential(nn.Linear(4,256), nn.LeakyReLU())\n",
    "        self.FC2 = nn.Sequential(nn.Linear(256,512), nn.LeakyReLU())\n",
    "        self.FC3 = nn.Sequential(nn.Linear(512,1024), nn.LeakyReLU())\n",
    "        self.FC4 = nn.Linear(1024,p)\n",
    "    def forward(self, s):\n",
    "#         隐含层层数为Ng\n",
    "        s = self.FC1(s)\n",
    "        s = self.FC2(s)\n",
    "        s = self.FC3(s)\n",
    "        s = self.FC4(s)\n",
    "        return nn.Softmax(dim=-1)(s)\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.FC1a = nn.Sequential(nn.Linear(10,256), nn.LeakyReLU())\n",
    "        self.FC1b = nn.Sequential(nn.Linear(4,256), nn.LeakyReLU())\n",
    "        self.FC2 = nn.Sequential(nn.Linear(512,512), nn.LeakyReLU())\n",
    "        self.FC3 = nn.Sequential(nn.Linear(512,256), nn.LeakyReLU())\n",
    "        self.FC4 = nn.Sequential(nn.Linear(256,1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, q, s):\n",
    "        # print(q.size())\n",
    "        # print(s.size())\n",
    "        s1a = self.FC1a(q)\n",
    "        s1b = self.FC1b(s)\n",
    "        s = torch.cat((s1a,s1b),-1)\n",
    "        # print(s.size())\n",
    "        s = self.FC2(s)\n",
    "        s = self.FC3(s)\n",
    "        s = self.FC4(s)\n",
    "        return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G(\n",
      "  (FC1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC2): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC3): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC4): Linear(in_features=1024, out_features=4, bias=True)\n",
      ")\n",
      "FC1.0.weight : torch.Size([256, 4])\n",
      "FC1.0.bias : torch.Size([256])\n",
      "FC2.0.weight : torch.Size([512, 256])\n",
      "FC2.0.bias : torch.Size([512])\n",
      "FC3.0.weight : torch.Size([1024, 512])\n",
      "FC3.0.bias : torch.Size([1024])\n",
      "FC4.weight : torch.Size([4, 1024])\n",
      "FC4.bias : torch.Size([4])\n",
      "tensor([[1., 1., 1., 1.]])\n",
      "tensor([[0.2704, 0.2420, 0.2414, 0.2462]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "在这里要\n",
    "1. 输出网络的结构(print,torch.summary两种)\n",
    "2. 查看网络的现有每层参数\n",
    "3. 构造数据，喂入网络，查看网络的输出\n",
    "'''\n",
    "\n",
    "g = G(4)\n",
    "print(g)\n",
    "for name,parameters in g.named_parameters():\n",
    "    print(name,':',parameters.size())\n",
    "#     print(parameters)\n",
    "\n",
    "a = torch.ones(1,4)\n",
    "print(a)\n",
    "print(g.forward(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 查看网络结构\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D(\n",
      "  (FC1a): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC1b): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC3): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC4): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "tensor([[1., 1., 1., 1.]]) tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 256])\n",
      "tensor([[0.5011]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "d = D()\n",
    "print(d)\n",
    "b = torch.ones(1,10)\n",
    "s = torch.ones(1,4)\n",
    "print(s,b)\n",
    "print(d.forward(b,s))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 对鉴别器D进行同样的测试\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           1,280\n",
      "         LeakyReLU-2               [-1, 1, 256]               0\n",
      "            Linear-3               [-1, 1, 512]         131,584\n",
      "         LeakyReLU-4               [-1, 1, 512]               0\n",
      "            Linear-5              [-1, 1, 1024]         525,312\n",
      "         LeakyReLU-6              [-1, 1, 1024]               0\n",
      "            Linear-7                [-1, 1, 10]          10,250\n",
      "           Softmax-8                [-1, 1, 10]               0\n",
      "================================================================\n",
      "Total params: 668,426\n",
      "Trainable params: 668,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 2.55\n",
      "Estimated Total Size (MB): 2.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g.cuda()\n",
    "summary(g,(1,4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 10])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 1, 256])\n",
      "torch.Size([2, 1, 256])\n",
      "torch.Size([2, 1, 512])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           2,816\n",
      "         LeakyReLU-2               [-1, 1, 256]               0\n",
      "            Linear-3               [-1, 1, 256]           1,280\n",
      "         LeakyReLU-4               [-1, 1, 256]               0\n",
      "            Linear-5               [-1, 1, 512]         262,656\n",
      "         LeakyReLU-6               [-1, 1, 512]               0\n",
      "            Linear-7               [-1, 1, 256]         131,328\n",
      "         LeakyReLU-8               [-1, 1, 256]               0\n",
      "            Linear-9                 [-1, 1, 1]             257\n",
      "          Sigmoid-10                 [-1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 398,337\n",
      "Trainable params: 398,337\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 1.52\n",
      "Estimated Total Size (MB): 1.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "d = D()\n",
    "d.cuda()\n",
    "summary(d,[(1,10),(1,4)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[[0.1046, 0.0961, 0.0924, 0.1008, 0.0950, 0.1078, 0.1037, 0.1027,\n",
      "          0.0999, 0.0970]],\n",
      "\n",
      "        [[0.1052, 0.0975, 0.0939, 0.0999, 0.0958, 0.1074, 0.1010, 0.1018,\n",
      "          0.1006, 0.0970]],\n",
      "\n",
      "        [[0.1042, 0.0995, 0.0939, 0.0994, 0.0961, 0.1060, 0.1014, 0.1025,\n",
      "          0.1000, 0.0970]]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor(3., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1\n",
      "tensor([[[0.1062, 0.0928, 0.0939, 0.1001, 0.0980, 0.1108, 0.1021, 0.0998,\n",
      "          0.0982, 0.0980]],\n",
      "\n",
      "        [[0.1026, 0.0988, 0.0965, 0.1012, 0.0978, 0.1046, 0.1027, 0.1018,\n",
      "          0.0989, 0.0951]],\n",
      "\n",
      "        [[0.1031, 0.0965, 0.0944, 0.1008, 0.0993, 0.1052, 0.1037, 0.1022,\n",
      "          0.0995, 0.0953]]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor(3., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "2\n",
      "tensor([[[0.1052, 0.0949, 0.0961, 0.1001, 0.0965, 0.1097, 0.1002, 0.0999,\n",
      "          0.0987, 0.0987]],\n",
      "\n",
      "        [[0.1022, 0.0956, 0.0948, 0.1013, 0.0985, 0.1064, 0.1047, 0.1009,\n",
      "          0.1005, 0.0951]],\n",
      "\n",
      "        [[0.1022, 0.0956, 0.0948, 0.1013, 0.0985, 0.1064, 0.1047, 0.1009,\n",
      "          0.1005, 0.0951]]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor(3., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "3\n",
      "tensor([[[0.1028, 0.0967, 0.0952, 0.1013, 0.0966, 0.1076, 0.1021, 0.1027,\n",
      "          0.1000, 0.0951]],\n",
      "\n",
      "        [[0.1042, 0.0995, 0.0939, 0.0994, 0.0961, 0.1060, 0.1014, 0.1025,\n",
      "          0.1000, 0.0970]],\n",
      "\n",
      "        [[0.1047, 0.0942, 0.0921, 0.1002, 0.0966, 0.1085, 0.1044, 0.1023,\n",
      "          0.1004, 0.0966]]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor(3.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "4\n",
      "tensor([[[0.1042, 0.0974, 0.0965, 0.1012, 0.0959, 0.1067, 0.1029, 0.1025,\n",
      "          0.0967, 0.0960]],\n",
      "\n",
      "        [[0.1048, 0.0956, 0.0946, 0.1010, 0.0985, 0.1060, 0.1037, 0.1000,\n",
      "          0.0985, 0.0974]],\n",
      "\n",
      "        [[0.1052, 0.0949, 0.0961, 0.1001, 0.0965, 0.1097, 0.1002, 0.0999,\n",
      "          0.0987, 0.0987]]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor(3., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "5\n",
      "tensor([[[0.1045, 0.0967, 0.0948, 0.0996, 0.0980, 0.1070, 0.1010, 0.1002,\n",
      "          0.0992, 0.0991]]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "g = G(10)\n",
    "g.cuda()\n",
    "for i,data in enumerate(dataloader, 0):\n",
    "    print(i)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    trainx = data[0].to(device)\n",
    "    y = g.forward(trainx)\n",
    "    print(y)\n",
    "    print(y.sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "D(\n  (FC1a): Sequential(\n    (0): Linear(in_features=10, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC1b): Sequential(\n    (0): Linear(in_features=4, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC2): Sequential(\n    (0): Linear(in_features=512, out_features=512, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC3): Sequential(\n    (0): Linear(in_features=512, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC4): Sequential(\n    (0): Linear(in_features=256, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step3. 对两个网络进行初始化，按照高斯分布初始化参数结构\n",
    "'''\n",
    "\n",
    "# 初始化函数接受一个初始化过的网络作为参数输入，将其参数重新初始化为高斯分布\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('FC') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "NetG = G(12)\n",
    "NetD = D()\n",
    "NetG.cuda()\n",
    "NetD.cuda()\n",
    "# 【标记】关于此处参数是否高斯分布生效需要进一步做验证\n",
    "NetG.apply(weights_init)\n",
    "NetD.apply(weights_init)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1.0.weight : Parameter containing:\n",
      "tensor([[-0.2813, -0.4514, -0.3028,  0.4451],\n",
      "        [ 0.1432,  0.1365,  0.3029, -0.3435],\n",
      "        [-0.2408,  0.0192,  0.1322, -0.2734],\n",
      "        ...,\n",
      "        [ 0.0901,  0.3240,  0.0126, -0.3547],\n",
      "        [-0.0486, -0.3292, -0.4434, -0.2826],\n",
      "        [ 0.4154,  0.1788,  0.0579, -0.0430]], requires_grad=True)\n",
      "FC1.0.bias : Parameter containing:\n",
      "tensor([-3.7050e-01, -2.8152e-01,  3.0605e-01, -3.4954e-01, -4.2278e-01,\n",
      "         4.2647e-01, -3.4497e-01,  2.5541e-01,  4.6757e-01, -2.9368e-01,\n",
      "         1.4880e-01, -7.6593e-03, -1.5279e-01, -2.5955e-01, -4.0801e-02,\n",
      "         7.5985e-02,  5.6768e-02,  3.9169e-01,  4.3593e-01, -2.5765e-01,\n",
      "        -7.0200e-02, -2.6391e-01, -4.7844e-01, -4.2165e-01,  2.3019e-02,\n",
      "        -4.8557e-01, -1.0182e-01,  1.6846e-01, -3.8121e-01,  3.9228e-01,\n",
      "         9.6484e-02, -2.7481e-01,  4.8452e-01,  1.7027e-01, -4.2788e-01,\n",
      "        -3.2793e-01,  3.3946e-01, -2.9105e-01,  2.9447e-01, -3.0274e-01,\n",
      "         1.0345e-01, -2.3761e-01,  3.9202e-01, -4.8818e-01, -3.4952e-01,\n",
      "        -3.7061e-01,  2.2859e-01, -4.2328e-01,  3.5276e-01, -7.7133e-02,\n",
      "        -2.0166e-01,  1.5886e-01, -2.9294e-01,  4.3959e-01,  4.0218e-01,\n",
      "         2.1425e-01, -4.3443e-01,  3.9573e-02,  2.0016e-01,  3.2968e-01,\n",
      "        -8.1883e-02, -1.9569e-01, -2.0990e-02,  2.9572e-01, -9.7575e-02,\n",
      "         2.1607e-01,  4.8008e-01, -2.8030e-01,  4.8700e-01, -1.6111e-03,\n",
      "         5.8997e-02,  4.8784e-01, -4.1108e-01, -1.7073e-01, -8.5711e-02,\n",
      "        -1.5764e-01,  3.5416e-01, -2.8999e-01,  1.6186e-01, -4.9344e-01,\n",
      "        -1.8230e-01,  1.3621e-02,  3.1707e-01,  1.2100e-02, -2.3842e-01,\n",
      "         1.3096e-01, -2.9506e-01,  1.2985e-01, -2.9854e-01, -5.8622e-02,\n",
      "         5.6178e-03,  1.4991e-01,  4.6859e-01,  2.3275e-01, -4.7996e-01,\n",
      "        -2.0298e-01, -3.7360e-04,  3.3383e-01,  2.1414e-01, -2.7832e-01,\n",
      "         4.8883e-01, -4.3491e-01, -2.7486e-01,  3.3717e-01,  2.2087e-01,\n",
      "         3.1727e-01,  2.0713e-01, -2.9376e-01, -3.0370e-01, -3.8466e-01,\n",
      "        -2.2686e-01,  1.1014e-01, -1.6710e-01,  1.1868e-01,  4.3934e-01,\n",
      "         2.2419e-01, -3.5903e-01, -1.6185e-01, -3.9119e-01,  1.8107e-02,\n",
      "        -1.0307e-01,  1.5909e-01, -4.4420e-01,  2.5262e-01, -2.1517e-01,\n",
      "         2.1526e-01,  3.1960e-01,  4.3226e-01,  4.0901e-01,  4.7290e-01,\n",
      "         3.4231e-01, -2.9236e-01, -8.6045e-02,  1.1016e-01, -3.6740e-01,\n",
      "         4.9160e-01,  3.6048e-02, -8.4007e-02,  1.8238e-01,  5.0040e-02,\n",
      "         3.1087e-01,  4.5027e-01, -4.4537e-01, -2.1211e-01,  7.7883e-02,\n",
      "        -3.1525e-01,  3.3889e-01,  3.0578e-01, -2.3066e-02, -1.4264e-01,\n",
      "         6.7799e-02, -3.6621e-01,  1.9375e-01, -1.5195e-01, -1.3549e-01,\n",
      "         7.2269e-02,  4.2306e-01, -2.6734e-01,  2.9663e-01,  4.8972e-01,\n",
      "         8.1064e-03, -3.1730e-02,  4.8280e-01, -1.1683e-01,  1.5702e-01,\n",
      "         1.4170e-01, -1.7266e-01, -4.9273e-01,  3.7033e-01, -1.6410e-01,\n",
      "        -3.9723e-01,  3.2841e-01, -4.9323e-01,  1.2772e-01, -2.9198e-01,\n",
      "        -3.7574e-01, -4.3464e-01, -4.7984e-01,  2.3893e-01,  1.3857e-02,\n",
      "         4.9001e-01,  2.6039e-01,  4.3509e-01,  4.0510e-02,  3.5248e-02,\n",
      "         4.9911e-01, -8.8509e-02, -2.9427e-01,  4.4969e-02, -4.8050e-01,\n",
      "         3.8952e-01, -2.5155e-01, -2.3698e-01,  1.9870e-01,  3.1202e-01,\n",
      "         4.0034e-01,  1.6298e-01,  4.6018e-01,  3.2363e-01, -1.9609e-01,\n",
      "        -3.3818e-01,  4.6260e-01, -3.0599e-01, -4.6257e-01,  1.1276e-01,\n",
      "        -2.8979e-01,  4.1088e-01, -4.9420e-01, -8.6888e-02,  1.7308e-02,\n",
      "         2.8614e-02,  1.3379e-02, -3.7638e-01, -4.0855e-01,  1.1477e-01,\n",
      "        -4.3604e-01,  2.6946e-01, -2.3770e-01,  3.7487e-01, -1.1630e-01,\n",
      "         1.9462e-01,  3.3785e-01,  4.3279e-01, -3.2314e-01, -5.9848e-02,\n",
      "        -5.5875e-02, -3.6009e-01, -1.5834e-01, -2.7622e-01,  2.1949e-02,\n",
      "         2.2648e-01, -7.7931e-02, -3.6579e-01, -2.5635e-01, -3.7045e-01,\n",
      "        -4.8794e-01,  2.4204e-01, -7.6170e-02, -3.4425e-01, -2.2617e-01,\n",
      "        -5.3208e-02,  4.0723e-01, -2.7193e-01, -1.0042e-02,  1.6058e-01,\n",
      "        -1.7018e-01, -1.3417e-01, -4.7484e-01,  4.6037e-01,  3.8102e-01,\n",
      "         4.4963e-01,  3.2930e-01, -4.3395e-01, -3.9016e-01,  4.8962e-01,\n",
      "        -4.2478e-01], requires_grad=True)\n",
      "FC2.0.weight : Parameter containing:\n",
      "tensor([[-0.0277,  0.0145, -0.0032,  ...,  0.0492, -0.0343,  0.0218],\n",
      "        [-0.0243, -0.0454, -0.0348,  ...,  0.0263,  0.0109, -0.0143],\n",
      "        [ 0.0055,  0.0049, -0.0046,  ..., -0.0154,  0.0560,  0.0469],\n",
      "        ...,\n",
      "        [-0.0081, -0.0004, -0.0545,  ..., -0.0188,  0.0440,  0.0156],\n",
      "        [-0.0322,  0.0495,  0.0235,  ..., -0.0408, -0.0568, -0.0095],\n",
      "        [ 0.0513, -0.0387,  0.0339,  ...,  0.0528, -0.0259,  0.0151]],\n",
      "       requires_grad=True)\n",
      "FC2.0.bias : Parameter containing:\n",
      "tensor([ 0.0166,  0.0039,  0.0253,  0.0080,  0.0167, -0.0481,  0.0247,  0.0617,\n",
      "         0.0279, -0.0518,  0.0477,  0.0153, -0.0481, -0.0459, -0.0614,  0.0399,\n",
      "        -0.0329, -0.0213,  0.0055,  0.0538, -0.0364,  0.0550,  0.0060,  0.0416,\n",
      "         0.0112, -0.0232,  0.0447, -0.0493, -0.0552, -0.0001, -0.0011, -0.0347,\n",
      "         0.0204,  0.0429,  0.0317, -0.0092, -0.0459, -0.0605,  0.0394, -0.0085,\n",
      "         0.0008, -0.0570,  0.0267, -0.0404,  0.0372,  0.0225, -0.0465, -0.0434,\n",
      "         0.0349,  0.0248, -0.0350,  0.0055,  0.0439,  0.0245,  0.0237, -0.0329,\n",
      "        -0.0504, -0.0191, -0.0450, -0.0382,  0.0558, -0.0474,  0.0183,  0.0125,\n",
      "         0.0135, -0.0498, -0.0299,  0.0234, -0.0568,  0.0388, -0.0602,  0.0160,\n",
      "         0.0368, -0.0155,  0.0278,  0.0479,  0.0159,  0.0141,  0.0277,  0.0112,\n",
      "         0.0031,  0.0162, -0.0012,  0.0543,  0.0207,  0.0328,  0.0454,  0.0452,\n",
      "         0.0180, -0.0259, -0.0307, -0.0154,  0.0452, -0.0556,  0.0025, -0.0412,\n",
      "        -0.0025, -0.0398,  0.0124, -0.0259,  0.0282, -0.0359, -0.0340,  0.0385,\n",
      "         0.0306,  0.0361, -0.0067,  0.0256, -0.0332, -0.0349,  0.0120,  0.0408,\n",
      "         0.0325,  0.0391, -0.0049,  0.0100,  0.0339, -0.0544, -0.0181,  0.0362,\n",
      "        -0.0219,  0.0122, -0.0597,  0.0517,  0.0422,  0.0368, -0.0039, -0.0516,\n",
      "        -0.0449, -0.0232, -0.0599, -0.0554,  0.0077, -0.0516, -0.0087, -0.0401,\n",
      "        -0.0304, -0.0577, -0.0502,  0.0502,  0.0436, -0.0289,  0.0599,  0.0307,\n",
      "        -0.0581, -0.0559, -0.0451, -0.0284, -0.0560, -0.0453, -0.0546,  0.0111,\n",
      "        -0.0473,  0.0002,  0.0211, -0.0213, -0.0552, -0.0251,  0.0119, -0.0016,\n",
      "         0.0352, -0.0469, -0.0049, -0.0304,  0.0366, -0.0400,  0.0368,  0.0521,\n",
      "         0.0060,  0.0615, -0.0147, -0.0181, -0.0129, -0.0391,  0.0324, -0.0114,\n",
      "         0.0108,  0.0071, -0.0243,  0.0042, -0.0534,  0.0371,  0.0503,  0.0017,\n",
      "        -0.0163,  0.0469, -0.0458,  0.0113,  0.0379,  0.0043,  0.0455,  0.0249,\n",
      "        -0.0319,  0.0480,  0.0551, -0.0194, -0.0409, -0.0545,  0.0200,  0.0276,\n",
      "        -0.0351,  0.0211,  0.0424, -0.0077, -0.0604, -0.0085, -0.0153,  0.0138,\n",
      "        -0.0335, -0.0552, -0.0244, -0.0454,  0.0118, -0.0584, -0.0548,  0.0605,\n",
      "        -0.0399,  0.0590,  0.0379,  0.0036, -0.0241,  0.0396,  0.0106, -0.0388,\n",
      "         0.0092, -0.0073, -0.0599, -0.0361,  0.0340,  0.0192, -0.0488, -0.0223,\n",
      "         0.0346,  0.0205,  0.0155,  0.0531,  0.0351,  0.0450,  0.0587,  0.0414,\n",
      "        -0.0193,  0.0459, -0.0507,  0.0020,  0.0619,  0.0262, -0.0113,  0.0321,\n",
      "        -0.0118,  0.0198, -0.0121, -0.0245,  0.0152,  0.0294, -0.0268,  0.0141,\n",
      "         0.0266, -0.0061, -0.0476,  0.0163, -0.0257,  0.0082, -0.0223,  0.0500,\n",
      "         0.0450,  0.0231, -0.0468,  0.0391, -0.0172, -0.0259,  0.0570, -0.0178,\n",
      "         0.0014,  0.0439, -0.0512, -0.0157,  0.0540,  0.0429,  0.0384,  0.0302,\n",
      "         0.0020,  0.0300,  0.0595, -0.0479, -0.0336, -0.0344, -0.0293,  0.0599,\n",
      "         0.0435, -0.0261,  0.0531, -0.0563,  0.0074, -0.0021,  0.0046, -0.0091,\n",
      "         0.0335,  0.0488, -0.0338,  0.0442,  0.0473, -0.0561, -0.0489, -0.0034,\n",
      "         0.0300, -0.0388,  0.0019, -0.0444, -0.0337, -0.0515,  0.0499,  0.0613,\n",
      "        -0.0335, -0.0334, -0.0598, -0.0207, -0.0368,  0.0577, -0.0617,  0.0331,\n",
      "        -0.0386, -0.0373,  0.0248,  0.0616, -0.0179, -0.0578, -0.0226,  0.0432,\n",
      "        -0.0602,  0.0172, -0.0127,  0.0089,  0.0255, -0.0280, -0.0410, -0.0596,\n",
      "        -0.0427,  0.0609,  0.0454, -0.0188,  0.0500, -0.0026, -0.0104, -0.0383,\n",
      "        -0.0113,  0.0024,  0.0456,  0.0395,  0.0291,  0.0020, -0.0597,  0.0175,\n",
      "        -0.0585, -0.0398,  0.0365,  0.0505, -0.0614,  0.0008,  0.0221, -0.0223,\n",
      "        -0.0078,  0.0129, -0.0263,  0.0087, -0.0319, -0.0068, -0.0542,  0.0510,\n",
      "         0.0547,  0.0100,  0.0189,  0.0551, -0.0298, -0.0396, -0.0576,  0.0312,\n",
      "        -0.0472, -0.0599, -0.0274,  0.0044, -0.0083, -0.0603,  0.0095,  0.0163,\n",
      "         0.0366, -0.0092, -0.0507, -0.0312, -0.0213,  0.0112,  0.0224,  0.0207,\n",
      "         0.0578,  0.0492,  0.0337,  0.0337, -0.0437, -0.0160,  0.0535, -0.0218,\n",
      "        -0.0568, -0.0162,  0.0037,  0.0058,  0.0061, -0.0206, -0.0012,  0.0499,\n",
      "        -0.0292, -0.0234, -0.0445,  0.0437,  0.0250,  0.0528, -0.0046, -0.0513,\n",
      "         0.0588, -0.0145, -0.0255,  0.0035, -0.0109,  0.0609, -0.0005,  0.0620,\n",
      "        -0.0335,  0.0201, -0.0486, -0.0331, -0.0328,  0.0525, -0.0104,  0.0458,\n",
      "         0.0196,  0.0267,  0.0281,  0.0302,  0.0067,  0.0488, -0.0011,  0.0142,\n",
      "         0.0153,  0.0360, -0.0589,  0.0600, -0.0143,  0.0013, -0.0281, -0.0475,\n",
      "        -0.0106,  0.0514, -0.0236, -0.0317,  0.0610, -0.0402,  0.0164, -0.0437,\n",
      "         0.0323,  0.0117, -0.0553, -0.0454,  0.0002, -0.0054,  0.0063,  0.0008,\n",
      "         0.0467, -0.0498,  0.0056, -0.0507,  0.0349, -0.0362,  0.0488, -0.0539,\n",
      "         0.0051,  0.0173, -0.0552,  0.0545,  0.0084, -0.0400, -0.0289,  0.0611,\n",
      "        -0.0009, -0.0476, -0.0064, -0.0397, -0.0577,  0.0394,  0.0552, -0.0175,\n",
      "        -0.0381,  0.0008,  0.0472, -0.0507, -0.0485, -0.0024,  0.0189,  0.0531,\n",
      "        -0.0018,  0.0515,  0.0605, -0.0162, -0.0102,  0.0202,  0.0329,  0.0156,\n",
      "        -0.0567,  0.0213,  0.0082, -0.0624, -0.0479, -0.0195, -0.0254, -0.0114],\n",
      "       requires_grad=True)\n",
      "FC3.0.weight : Parameter containing:\n",
      "tensor([[ 1.2827e-02, -1.3388e-02, -2.4537e-03,  ..., -1.4488e-02,\n",
      "         -2.1106e-02, -3.0003e-02],\n",
      "        [ 3.2932e-02, -2.9429e-02,  2.8132e-02,  ..., -4.5952e-03,\n",
      "          4.0702e-03,  2.6593e-02],\n",
      "        [ 5.3155e-03, -1.9502e-02, -2.6556e-02,  ...,  6.9264e-04,\n",
      "          4.0430e-02,  8.4284e-03],\n",
      "        ...,\n",
      "        [-4.3833e-02, -3.7644e-02, -8.5432e-05,  ..., -3.0942e-02,\n",
      "          3.1937e-03,  2.5737e-03],\n",
      "        [ 2.6761e-02,  3.1417e-02, -5.6456e-03,  ...,  3.2202e-02,\n",
      "          3.0295e-02, -3.1074e-02],\n",
      "        [ 3.9169e-02,  4.2699e-02,  1.3497e-02,  ...,  4.3640e-02,\n",
      "          3.2729e-02,  3.5541e-02]], requires_grad=True)\n",
      "FC3.0.bias : Parameter containing:\n",
      "tensor([ 0.0250,  0.0040,  0.0219,  ...,  0.0415,  0.0172, -0.0388],\n",
      "       requires_grad=True)\n",
      "FC4.weight : Parameter containing:\n",
      "tensor([[ 0.0040,  0.0239,  0.0234,  ...,  0.0252,  0.0003, -0.0048],\n",
      "        [-0.0024, -0.0004, -0.0282,  ..., -0.0280, -0.0063, -0.0185],\n",
      "        [ 0.0231,  0.0077, -0.0198,  ..., -0.0193, -0.0187,  0.0147],\n",
      "        [-0.0110, -0.0052, -0.0277,  ...,  0.0089,  0.0222,  0.0101]],\n",
      "       requires_grad=True)\n",
      "FC4.bias : Parameter containing:\n",
      "tensor([ 0.0080,  0.0026, -0.0219,  0.0032], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in g.named_parameters():\n",
    "    print(name,':',parameters)\n",
    "#     print(parameters)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 查看网络参数\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "'''\n",
    "Step4. 分别定义两个网络的损失函数和优化方法，进行训练\n",
    "'''\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "# 这里产生一个固定的噪声，每轮学习完成后都使用固定噪声来进行输出\n",
    "# fixed_noise = torch.randn(25, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "optimizerD = torch.optim.Adam(NetD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(NetG.parameters(), lr=lr, betas=(beta1, 0.999))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7234\tD(x): 0.4851\tD(G(z)): 0.4851 / 0.4851\n",
      "[1/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7240\tD(x): 0.4848\tD(G(z)): 0.4848 / 0.4848\n",
      "[2/25][0/6]\tLoss_D: 1.3871\tLoss_G: 0.7215\tD(x): 0.4860\tD(G(z)): 0.4860 / 0.4860\n",
      "[3/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7234\tD(x): 0.4851\tD(G(z)): 0.4851 / 0.4851\n",
      "[4/25][0/6]\tLoss_D: 1.3870\tLoss_G: 0.7209\tD(x): 0.4863\tD(G(z)): 0.4863 / 0.4863\n",
      "[5/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7232\tD(x): 0.4852\tD(G(z)): 0.4852 / 0.4852\n",
      "[6/25][0/6]\tLoss_D: 1.3870\tLoss_G: 0.7209\tD(x): 0.4863\tD(G(z)): 0.4863 / 0.4863\n",
      "[7/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7228\tD(x): 0.4854\tD(G(z)): 0.4854 / 0.4854\n",
      "[8/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7241\tD(x): 0.4848\tD(G(z)): 0.4848 / 0.4848\n",
      "[9/25][0/6]\tLoss_D: 1.3873\tLoss_G: 0.7245\tD(x): 0.4846\tD(G(z)): 0.4846 / 0.4846\n",
      "[10/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7245\tD(x): 0.4846\tD(G(z)): 0.4846 / 0.4846\n",
      "[11/25][0/6]\tLoss_D: 1.3871\tLoss_G: 0.7222\tD(x): 0.4857\tD(G(z)): 0.4857 / 0.4857\n",
      "[12/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7231\tD(x): 0.4852\tD(G(z)): 0.4852 / 0.4852\n",
      "[13/25][0/6]\tLoss_D: 1.3871\tLoss_G: 0.7220\tD(x): 0.4858\tD(G(z)): 0.4858 / 0.4858\n",
      "[14/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7239\tD(x): 0.4849\tD(G(z)): 0.4849 / 0.4849\n",
      "[15/25][0/6]\tLoss_D: 1.3873\tLoss_G: 0.7247\tD(x): 0.4845\tD(G(z)): 0.4845 / 0.4845\n",
      "[16/25][0/6]\tLoss_D: 1.3871\tLoss_G: 0.7217\tD(x): 0.4859\tD(G(z)): 0.4859 / 0.4859\n",
      "[17/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7239\tD(x): 0.4848\tD(G(z)): 0.4848 / 0.4848\n",
      "[18/25][0/6]\tLoss_D: 1.3871\tLoss_G: 0.7217\tD(x): 0.4859\tD(G(z)): 0.4859 / 0.4859\n",
      "[19/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7242\tD(x): 0.4847\tD(G(z)): 0.4847 / 0.4847\n",
      "[20/25][0/6]\tLoss_D: 1.3873\tLoss_G: 0.7249\tD(x): 0.4844\tD(G(z)): 0.4844 / 0.4844\n",
      "[21/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7229\tD(x): 0.4853\tD(G(z)): 0.4853 / 0.4853\n",
      "[22/25][0/6]\tLoss_D: 1.3872\tLoss_G: 0.7228\tD(x): 0.4854\tD(G(z)): 0.4854 / 0.4854\n",
      "[23/25][0/6]\tLoss_D: 1.3873\tLoss_G: 0.7246\tD(x): 0.4845\tD(G(z)): 0.4845 / 0.4845\n",
      "[24/25][0/6]\tLoss_D: 1.3873\tLoss_G: 0.7249\tD(x): 0.4844\tD(G(z)): 0.4844 / 0.4844\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step5. 开始进行网络训练\n",
    "首先对鉴别器进行训练，然后训练生成器\n",
    "'''\n",
    "# Training Loop\n",
    "num_epochs = 25\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "ngpu = 1\n",
    "nz = 100\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "# dataset = NumbersDataset(0, 50)\n",
    "# dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        NetD.zero_grad()\n",
    "        # Format batch\n",
    "        data_train = data[0].to(device)\n",
    "        data_label = data[1].to(device)\n",
    "\n",
    "        batch_size = data_train.size(0)\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "\n",
    "        # Forward pass real batch through D\n",
    "        output = NetD(data_label, data_train).view(-1)\n",
    "        # print(output.size())\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        # noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        p = NetG(data_train)\n",
    "        noise_lable = fakeFunction(batch_size, p)\n",
    "        noise_lable.to(device)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = NetD(noise_lable, data_train).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        NetG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = NetD(noise_lable, data_train).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}