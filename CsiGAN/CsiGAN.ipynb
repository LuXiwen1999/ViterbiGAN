{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% 使用GAN生成信道转移向量，完成训练\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n发送长度为K的导频符号序列，其中发送符号x[i]从发送集S中等概率随机选取，接收端接受y[i]\\n训练数据即T[i]=(S[i],y[i])\\n收集多个i时刻的信道输入状态向量与信道输出（K个），从而得到训练数据集T=[T1,T2,...Ti]\\n'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "发送长度为K的导频符号序列，其中发送符号x[i]从发送集S中等概率随机选取，接收端接受y[i]\n",
    "训练数据即T[i]=(S[i],y[i])\n",
    "收集多个i时刻的信道输入状态向量与信道输出（K个），从而得到训练数据集T=[T1,T2,...Ti]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step1. 开始构建数据生成器，产生S[i]与y[i]的数据对\n",
    "根据后面的看法，似乎所有的观测值有一个特定的集合，y[i]只会在特定集合内获取\n",
    "最终的网络输出相当于成为了一个分类问题\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NumbersDataset(Dataset):\n",
    "    def __init__(self, low, high):\n",
    "        self.samples = list(range(low, high))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transmitter = self.samples[idx]\n",
    "        receiver = transmitter + 1\n",
    "        return transmitter, receiver"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "(5, 6)\n",
      "[tensor([15, 11, 12, 14, 19, 43, 22, 17, 23,  4]), tensor([16, 12, 13, 15, 20, 44, 23, 18, 24,  5])]\n"
     ]
    }
   ],
   "source": [
    "dataset = NumbersDataset(0,50)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "print(len(dataset))\n",
    "print(dataset[5])\n",
    "print(next(iter(dataloader)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 构建关于数据生成器的测试\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step2. 开始构建两个全连接神经网络D和G，使参数满足标准高斯分布\n",
    "所述的生成器G是一个全连接神经网络\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(G, self).__init__()\n",
    "        self.FC1 = nn.Sequential(nn.Linear(4,256), nn.LeakyReLU())\n",
    "        self.FC2 = nn.Sequential(nn.Linear(256,512), nn.LeakyReLU())\n",
    "        self.FC3 = nn.Sequential(nn.Linear(512,1024), nn.LeakyReLU())\n",
    "        self.FC4 = nn.Linear(1024,p)\n",
    "    def forward(self, s):\n",
    "#         隐含层层数为Ng\n",
    "        s = self.FC1(s)\n",
    "        s = self.FC2(s)\n",
    "        s = self.FC3(s)\n",
    "        s = self.FC4(s)\n",
    "        return nn.Softmax(s)\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.FC1a = nn.Sequential(nn.Linear(10,256), nn.LeakyReLU())\n",
    "        self.FC1b = nn.Sequential(nn.Linear(4,256), nn.LeakyReLU())\n",
    "        self.FC2 = nn.Sequential(nn.Linear(512,512), nn.LeakyReLU())\n",
    "        self.FC3 = nn.Sequential(nn.Linear(512,256), nn.LeakyReLU())\n",
    "        self.FC4 = nn.Sequential(nn.Linear(256,1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, q, s):\n",
    "        s1a = self.FC1a(q)\n",
    "        s1b = self.FC1b(s)\n",
    "        s = torch.cat((s1a,s1b),1)\n",
    "        s = self.FC2(s)\n",
    "        s = self.FC3(s)\n",
    "        s = self.FC4(s)\n",
    "        return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G(\n",
      "  (FC1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC2): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC3): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC4): Linear(in_features=1024, out_features=4, bias=True)\n",
      ")\n",
      "FC1.0.weight : torch.Size([256, 4])\n",
      "FC1.0.bias : torch.Size([256])\n",
      "FC2.0.weight : torch.Size([512, 256])\n",
      "FC2.0.bias : torch.Size([512])\n",
      "FC3.0.weight : torch.Size([1024, 512])\n",
      "FC3.0.bias : torch.Size([1024])\n",
      "FC4.weight : torch.Size([4, 1024])\n",
      "FC4.bias : torch.Size([4])\n",
      "tensor([[1., 1., 1., 1.]])\n",
      "Softmax(dim=tensor([[-0.0199, -0.0062, -0.0274, -0.0791]], grad_fn=<AddmmBackward>))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "在这里要\n",
    "1. 输出网络的结构(print,torch.summary两种)\n",
    "2. 查看网络的现有每层参数\n",
    "3. 构造数据，喂入网络，查看网络的输出\n",
    "'''\n",
    "\n",
    "g = G(4)\n",
    "print(g)\n",
    "for name,parameters in g.named_parameters():\n",
    "    print(name,':',parameters.size())\n",
    "#     print(parameters)\n",
    "\n",
    "a = torch.ones(1,4)\n",
    "print(a)\n",
    "print(g.forward(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 查看网络结构\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D(\n",
      "  (FC1a): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC1b): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC3): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (FC4): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "tensor([[1., 1., 1., 1.]]) tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.4842]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "d = D()\n",
    "print(d)\n",
    "b = torch.ones(1,10)\n",
    "s = torch.ones(1,4)\n",
    "print(s,b)\n",
    "print(d.forward(b,s))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 对鉴别器D进行同样的测试\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           1,280\n",
      "         LeakyReLU-2               [-1, 1, 256]               0\n",
      "            Linear-3               [-1, 1, 512]         131,584\n",
      "         LeakyReLU-4               [-1, 1, 512]               0\n",
      "            Linear-5              [-1, 1, 1024]         525,312\n",
      "         LeakyReLU-6              [-1, 1, 1024]               0\n",
      "            Linear-7                 [-1, 1, 4]           4,100\n",
      "================================================================\n",
      "Total params: 662,276\n",
      "Trainable params: 662,276\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 2.53\n",
      "Estimated Total Size (MB): 2.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g.cuda()\n",
    "summary(g,(1,4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "D(\n  (FC1a): Sequential(\n    (0): Linear(in_features=10, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC1b): Sequential(\n    (0): Linear(in_features=4, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC2): Sequential(\n    (0): Linear(in_features=512, out_features=512, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC3): Sequential(\n    (0): Linear(in_features=512, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (FC4): Linear(in_features=256, out_features=1, bias=True)\n)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step3. 对两个网络进行初始化，按照高斯分布初始化参数结构\n",
    "'''\n",
    "\n",
    "# 初始化函数接受一个初始化过的网络作为参数输入，将其参数重新初始化为高斯分布\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('FC') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "NetG = G(4)\n",
    "NetD = D()\n",
    "\n",
    "# 【标记】关于此处参数是否高斯分布生效需要进一步做验证\n",
    "NetG.apply(weights_init)\n",
    "NetD.apply(weights_init)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1.0.weight : Parameter containing:\n",
      "tensor([[ 0.2476,  0.3361, -0.0224,  0.3665],\n",
      "        [-0.3288, -0.0021, -0.1355,  0.1622],\n",
      "        [-0.0845, -0.3688,  0.1261, -0.0074],\n",
      "        ...,\n",
      "        [ 0.0375,  0.4836, -0.1558, -0.3141],\n",
      "        [-0.0047,  0.0259,  0.3597,  0.2185],\n",
      "        [ 0.1458,  0.3071, -0.1539,  0.3910]], requires_grad=True)\n",
      "FC1.0.bias : Parameter containing:\n",
      "tensor([-0.1422,  0.1799,  0.1027, -0.1581,  0.4635, -0.0596,  0.1315,  0.1170,\n",
      "        -0.1837, -0.0923,  0.3385, -0.1950,  0.4048,  0.2080,  0.2331, -0.0882,\n",
      "        -0.3681, -0.0169,  0.4692, -0.0214,  0.2953,  0.1764,  0.2863, -0.1139,\n",
      "         0.1676,  0.3971, -0.1315,  0.3523, -0.3366,  0.4494,  0.2553, -0.0963,\n",
      "         0.4506,  0.3084, -0.4688,  0.3498, -0.2792,  0.0443, -0.1918,  0.0030,\n",
      "        -0.2830,  0.1134, -0.2853, -0.3767,  0.0801,  0.3050, -0.0069,  0.0238,\n",
      "         0.1729,  0.3697, -0.2299, -0.2623, -0.3445,  0.3331, -0.3351,  0.1995,\n",
      "        -0.3477, -0.2402, -0.3698, -0.4327,  0.3517, -0.3253, -0.0963,  0.0743,\n",
      "         0.0385,  0.0539,  0.3460, -0.0781, -0.2911, -0.1749, -0.3574,  0.0410,\n",
      "        -0.0571,  0.3783, -0.2046,  0.2533,  0.0902,  0.0419, -0.4494, -0.0079,\n",
      "        -0.3339,  0.4125,  0.3092,  0.1979,  0.1172,  0.0938, -0.3946,  0.4887,\n",
      "         0.4741,  0.1089,  0.2444,  0.2124, -0.4902,  0.2893,  0.0827, -0.3914,\n",
      "        -0.3174,  0.1088,  0.0668, -0.2112,  0.2812, -0.1017,  0.4811, -0.2854,\n",
      "         0.3606,  0.3031, -0.0561, -0.4354, -0.3152,  0.3854,  0.3339, -0.1144,\n",
      "         0.1501, -0.4452, -0.2194,  0.3075, -0.4522, -0.2077,  0.3963,  0.4888,\n",
      "         0.0759, -0.3862,  0.0821, -0.2815, -0.0655, -0.4285, -0.1835, -0.3191,\n",
      "        -0.3610,  0.2156,  0.1046,  0.3516, -0.0409, -0.3355, -0.2162, -0.0540,\n",
      "         0.0372,  0.4628,  0.3516, -0.0068, -0.2578,  0.2306, -0.0647,  0.3787,\n",
      "        -0.0206, -0.3701,  0.0983,  0.3717,  0.2441, -0.4767, -0.3711, -0.2533,\n",
      "        -0.2787, -0.1179, -0.2402, -0.4433, -0.2943,  0.4333,  0.0567, -0.3046,\n",
      "         0.4746,  0.4156,  0.2505,  0.1833, -0.2912, -0.0449, -0.3191,  0.2632,\n",
      "         0.3785,  0.3306, -0.1825,  0.4972,  0.4331,  0.2738, -0.1133, -0.2458,\n",
      "        -0.3764,  0.4918, -0.1830,  0.1524, -0.0197,  0.2362,  0.0261,  0.2113,\n",
      "        -0.3045,  0.3236, -0.2987,  0.2109, -0.4932,  0.4743,  0.4184,  0.3980,\n",
      "        -0.2009, -0.0072, -0.3786, -0.1359,  0.2823,  0.2601,  0.3357, -0.0153,\n",
      "        -0.0594,  0.1899,  0.3521, -0.0242, -0.2656,  0.4963,  0.2825, -0.1381,\n",
      "        -0.1889, -0.4814,  0.4807, -0.3643,  0.1792,  0.2749, -0.3879, -0.2284,\n",
      "        -0.2357, -0.1485, -0.0889, -0.3239,  0.1382,  0.3335,  0.3956, -0.0478,\n",
      "         0.0797, -0.0285,  0.3487, -0.2726, -0.0089,  0.1687, -0.0534,  0.0924,\n",
      "         0.2847,  0.3868,  0.1963,  0.1652, -0.0439,  0.2433,  0.3627,  0.0104,\n",
      "        -0.3043,  0.3326, -0.0314,  0.3116, -0.1300,  0.4154,  0.3281, -0.0721,\n",
      "         0.1935, -0.1630,  0.3168, -0.0462, -0.3555, -0.3378, -0.3832, -0.0984],\n",
      "       requires_grad=True)\n",
      "FC2.0.weight : Parameter containing:\n",
      "tensor([[-0.0383,  0.0224,  0.0137,  ...,  0.0584, -0.0084,  0.0551],\n",
      "        [ 0.0235,  0.0532,  0.0087,  ..., -0.0269, -0.0252, -0.0346],\n",
      "        [-0.0115,  0.0035, -0.0093,  ...,  0.0216,  0.0262, -0.0098],\n",
      "        ...,\n",
      "        [-0.0514,  0.0187,  0.0126,  ..., -0.0525, -0.0576, -0.0127],\n",
      "        [ 0.0524, -0.0428, -0.0393,  ...,  0.0116, -0.0127,  0.0077],\n",
      "        [ 0.0619, -0.0006,  0.0064,  ..., -0.0441, -0.0144,  0.0498]],\n",
      "       requires_grad=True)\n",
      "FC2.0.bias : Parameter containing:\n",
      "tensor([-5.7096e-02, -3.1353e-02,  5.5641e-02, -1.4762e-02,  4.3163e-02,\n",
      "        -5.1471e-02,  2.0949e-02,  3.1085e-02,  9.1785e-03,  1.3025e-02,\n",
      "         4.7638e-02, -1.3377e-02,  1.4323e-02, -3.1981e-03, -2.2186e-03,\n",
      "         1.2538e-04,  2.6195e-02,  9.0663e-04,  1.6634e-02, -6.1958e-02,\n",
      "        -9.6520e-03,  7.5544e-03, -1.6254e-02,  2.6919e-02, -3.1028e-02,\n",
      "         5.1978e-02, -4.1625e-02,  1.2726e-02, -1.7059e-03,  3.7018e-02,\n",
      "        -2.5786e-02, -3.1223e-02,  1.9178e-02, -5.2523e-02,  3.9599e-04,\n",
      "         8.2876e-03, -1.4585e-02,  5.2098e-02, -2.9999e-02,  8.4601e-03,\n",
      "        -2.5307e-02, -1.9891e-02,  1.1838e-02,  3.6248e-03, -4.0483e-02,\n",
      "        -2.1770e-03, -3.9738e-02,  2.5577e-02,  1.3324e-02,  1.3214e-02,\n",
      "        -1.1003e-02,  5.6880e-02, -2.2913e-02,  5.4186e-02, -5.2427e-02,\n",
      "        -4.2713e-02,  1.0655e-02,  8.0603e-03, -5.1272e-02,  2.9191e-02,\n",
      "        -1.0785e-02, -2.4251e-02, -1.0101e-02, -1.3086e-02, -2.6834e-02,\n",
      "         8.7633e-03, -3.1330e-02,  4.2495e-02,  3.9469e-02, -1.9803e-02,\n",
      "         5.6349e-02,  4.1268e-02, -2.4415e-02,  1.3921e-03, -3.2338e-02,\n",
      "        -2.0383e-02,  2.7213e-02, -1.7483e-02,  5.9267e-02,  8.0364e-03,\n",
      "         1.9297e-02, -3.1467e-02, -1.4655e-03,  4.4219e-02, -5.5374e-02,\n",
      "         4.3624e-02,  4.9534e-02, -2.9774e-02,  2.8161e-02, -4.7277e-02,\n",
      "        -2.4023e-02, -4.9575e-02,  4.2289e-02,  2.5772e-02,  5.8654e-03,\n",
      "        -5.5687e-02, -4.3211e-02,  5.7215e-02, -2.5037e-03, -8.0831e-03,\n",
      "         5.0257e-02, -4.7882e-02, -4.0572e-02,  1.6439e-02, -2.2802e-02,\n",
      "         1.6335e-02, -5.4720e-03,  4.3863e-02, -9.6966e-03,  2.9272e-02,\n",
      "        -5.2163e-02,  4.3201e-02,  9.6882e-03, -5.5224e-02, -2.6367e-02,\n",
      "         4.4026e-02,  5.8799e-02, -2.8409e-02, -4.5968e-02,  2.6758e-02,\n",
      "         4.9560e-02,  1.3315e-02, -2.7345e-03,  1.3368e-02,  2.2035e-02,\n",
      "         1.7091e-02,  2.4035e-02, -3.2485e-02,  2.2784e-02, -2.0108e-02,\n",
      "        -2.1439e-03,  2.1348e-02,  2.0092e-02, -4.5463e-02,  2.7148e-02,\n",
      "         1.6025e-02, -4.0538e-02, -3.6385e-02, -2.9929e-02,  8.7611e-04,\n",
      "         5.9023e-02, -8.9166e-03,  6.2190e-02,  2.2168e-02, -5.8683e-02,\n",
      "        -5.2242e-02, -3.1601e-02,  5.3400e-02,  1.2796e-02,  2.2724e-02,\n",
      "        -6.2073e-02,  2.3381e-02, -2.4403e-02,  2.9646e-02,  5.4417e-04,\n",
      "        -1.5969e-02, -4.8696e-02, -5.8497e-02,  3.7588e-02,  5.7541e-02,\n",
      "        -2.5102e-02,  2.2144e-02,  2.1996e-04,  4.3540e-02, -6.1381e-02,\n",
      "         4.7575e-02,  6.6910e-03, -2.3365e-02,  4.0184e-02,  2.3620e-02,\n",
      "         3.7227e-02, -1.5230e-02,  4.5299e-02, -4.7223e-03,  5.6504e-02,\n",
      "        -4.3848e-02,  3.1229e-02, -4.3176e-02,  1.5004e-02,  1.3419e-03,\n",
      "        -6.2558e-03, -2.0174e-02,  5.6478e-02,  4.4859e-02, -5.6108e-02,\n",
      "        -3.9887e-02, -2.2104e-02,  2.9164e-02,  2.7435e-02, -4.3053e-02,\n",
      "        -2.2571e-02,  4.6470e-02, -4.5970e-02, -4.8303e-02,  4.4337e-02,\n",
      "        -1.4559e-02, -1.7122e-02, -4.0315e-02, -5.0049e-02,  2.8842e-02,\n",
      "        -3.4629e-02, -3.8539e-02,  5.4384e-03, -3.8913e-02, -3.4966e-02,\n",
      "         8.2408e-03,  3.0467e-02, -2.8545e-02,  4.7667e-02,  2.2056e-02,\n",
      "        -2.1554e-02,  1.6438e-02,  1.2527e-02, -1.7049e-02,  2.3503e-02,\n",
      "         5.3731e-02, -5.0182e-02,  1.0409e-02, -4.7205e-03,  1.1158e-02,\n",
      "        -6.0182e-02, -1.8238e-02, -7.6778e-05,  5.8672e-03,  5.0675e-02,\n",
      "         1.0165e-02, -7.9856e-03, -4.5925e-02, -1.4622e-02, -1.7899e-02,\n",
      "        -1.6551e-02,  5.1116e-02,  5.3041e-02, -1.1409e-02, -2.3939e-03,\n",
      "        -4.6630e-02, -1.5035e-02,  3.6262e-02, -3.8494e-02,  3.9982e-02,\n",
      "         2.8906e-03, -4.8546e-02, -5.5108e-02, -6.2519e-03, -4.8204e-02,\n",
      "         4.2023e-02,  4.4757e-02, -3.3991e-02,  4.0985e-02,  5.9585e-02,\n",
      "        -2.9140e-02,  2.4250e-02,  1.2388e-02, -2.0482e-02, -9.8651e-03,\n",
      "         4.9176e-02,  7.7464e-04,  4.2544e-02, -5.0538e-02,  6.0146e-02,\n",
      "         5.4994e-02,  3.1896e-03, -5.5171e-02, -5.9028e-02,  5.7454e-02,\n",
      "         3.0149e-02,  2.9648e-02, -3.4424e-02, -3.4484e-02,  4.1417e-02,\n",
      "        -8.6329e-03, -6.0438e-02, -3.4667e-02,  2.5553e-02,  3.9640e-02,\n",
      "         3.0987e-02,  3.3634e-02, -5.4835e-02, -3.3523e-02, -1.6748e-02,\n",
      "        -1.3032e-02,  1.2523e-02,  6.6751e-03,  1.3087e-03,  5.4978e-02,\n",
      "         3.8906e-02, -3.2416e-02,  6.1180e-02,  2.1811e-02,  1.0684e-02,\n",
      "        -3.2501e-02,  3.8994e-03,  3.2604e-02,  2.6239e-02, -5.8110e-02,\n",
      "        -8.3783e-03, -5.4353e-02, -4.5344e-02,  6.8414e-03,  7.7678e-03,\n",
      "         4.6344e-02,  4.2761e-02,  2.8254e-02,  3.6701e-02, -4.9109e-02,\n",
      "        -6.2467e-02,  2.0150e-02, -2.9423e-02, -2.1525e-02, -5.9822e-03,\n",
      "        -2.9559e-02,  9.6408e-03,  2.2296e-02,  4.8526e-02,  5.5435e-02,\n",
      "        -4.0901e-02, -2.9546e-02,  5.9790e-02,  5.5049e-02, -8.2865e-03,\n",
      "         1.2604e-02,  8.8523e-03, -5.3817e-02, -5.9262e-03,  2.0814e-02,\n",
      "        -4.2080e-02, -2.9609e-02,  5.0054e-02, -4.8235e-02, -5.6214e-02,\n",
      "        -7.3401e-03,  3.8421e-02,  3.1241e-03,  5.0806e-02, -2.5691e-03,\n",
      "         1.3908e-02,  3.2565e-03, -2.4020e-02,  6.8021e-03, -3.1768e-02,\n",
      "        -2.4609e-02,  4.5201e-02, -4.7914e-02,  4.6349e-02, -4.2993e-02,\n",
      "        -2.7257e-02,  5.9676e-02,  3.3526e-02, -3.8986e-02, -1.1595e-02,\n",
      "         3.0549e-02,  4.8116e-02, -6.4786e-03,  2.9196e-02,  3.6115e-02,\n",
      "        -2.0758e-03,  5.0303e-02, -4.0376e-03, -3.5380e-02,  4.4356e-02,\n",
      "        -1.5674e-02, -1.2226e-02,  4.8097e-03,  4.4764e-02, -2.9887e-02,\n",
      "         5.6665e-02,  2.2459e-02,  6.0314e-02,  2.7891e-02, -1.8496e-02,\n",
      "         5.5800e-02,  4.0093e-02,  5.7851e-02,  4.0721e-02, -4.1355e-02,\n",
      "        -6.2026e-02,  4.5070e-02,  5.4443e-02,  2.7823e-02, -2.5777e-03,\n",
      "         1.8004e-02, -7.4916e-04, -2.9911e-02,  3.1530e-03, -3.5818e-02,\n",
      "         4.2505e-02, -3.3599e-02,  4.0023e-02,  9.1786e-03, -1.5725e-02,\n",
      "         6.6000e-03, -2.4965e-02, -2.1061e-02, -1.5836e-02,  4.4055e-02,\n",
      "        -4.2858e-02, -5.2199e-02, -7.0549e-03,  1.9208e-02, -3.4585e-03,\n",
      "         3.3457e-02,  3.8349e-02,  4.1163e-02, -9.4325e-03, -2.1809e-03,\n",
      "        -4.2327e-03,  2.8716e-02, -5.2773e-02, -5.3357e-02, -5.8388e-02,\n",
      "        -3.7006e-02,  5.5471e-02, -1.0207e-02,  5.1892e-02, -2.9571e-02,\n",
      "         1.4012e-02, -3.3006e-02,  4.9536e-02, -1.9222e-02,  1.6798e-02,\n",
      "        -4.3168e-02,  3.7076e-03,  4.7095e-05,  5.4389e-02,  8.5393e-03,\n",
      "         6.1552e-02,  4.8435e-03, -1.6244e-02, -5.9555e-02,  1.5189e-02,\n",
      "         5.9890e-03, -2.2888e-03,  3.3501e-02, -4.3313e-02, -4.2291e-02,\n",
      "         9.6947e-03,  3.6080e-02, -2.0296e-03,  4.5879e-02,  3.2772e-02,\n",
      "         3.8339e-02, -3.7783e-02, -2.5094e-02,  1.8590e-02,  4.8018e-02,\n",
      "        -2.9233e-02, -8.6873e-03,  3.5269e-02,  2.3337e-02, -1.9527e-02,\n",
      "        -9.2067e-04, -3.1665e-02,  3.6749e-03,  3.6672e-02, -1.3710e-02,\n",
      "         5.9355e-02, -1.5650e-02,  1.2010e-02, -4.1636e-02,  3.7733e-02,\n",
      "        -3.5995e-02, -5.8869e-02, -3.3235e-02, -2.2372e-02, -1.9497e-02,\n",
      "         3.5568e-03, -1.5023e-02, -1.1641e-02,  4.9990e-03, -5.5009e-02,\n",
      "        -5.8913e-02,  8.3467e-03, -2.4538e-02, -3.6760e-02, -1.2502e-02,\n",
      "        -1.9441e-02,  8.0633e-03, -4.6818e-02,  2.7267e-02, -1.7712e-02,\n",
      "        -6.1691e-02,  1.5017e-02, -4.7354e-02,  2.0581e-02,  6.2116e-02,\n",
      "         5.1018e-02,  3.1713e-02, -6.1593e-02, -1.1143e-02,  3.8608e-02,\n",
      "         2.9392e-02,  4.8152e-02,  3.6207e-02, -3.8192e-02, -1.7257e-02,\n",
      "        -5.9680e-02,  2.5272e-02,  1.1518e-02, -3.8883e-02,  1.7579e-02,\n",
      "         3.0436e-02,  1.3375e-02, -5.1812e-02, -2.5580e-02,  1.0927e-02,\n",
      "         3.3761e-02, -3.4235e-02,  1.8807e-02,  1.6050e-02, -5.5227e-02,\n",
      "        -1.8412e-02, -1.3818e-02], requires_grad=True)\n",
      "FC3.0.weight : Parameter containing:\n",
      "tensor([[ 0.0033, -0.0398,  0.0343,  ...,  0.0138,  0.0346, -0.0320],\n",
      "        [-0.0266,  0.0076, -0.0192,  ...,  0.0124, -0.0242, -0.0420],\n",
      "        [ 0.0150,  0.0372,  0.0253,  ...,  0.0388, -0.0360, -0.0070],\n",
      "        ...,\n",
      "        [ 0.0324,  0.0137, -0.0227,  ...,  0.0212, -0.0421, -0.0022],\n",
      "        [-0.0042,  0.0249, -0.0167,  ..., -0.0307,  0.0337,  0.0044],\n",
      "        [ 0.0057,  0.0340, -0.0440,  ...,  0.0354, -0.0077,  0.0164]],\n",
      "       requires_grad=True)\n",
      "FC3.0.bias : Parameter containing:\n",
      "tensor([ 0.0144,  0.0234, -0.0254,  ..., -0.0332, -0.0048,  0.0158],\n",
      "       requires_grad=True)\n",
      "FC4.weight : Parameter containing:\n",
      "tensor([[ 0.0071,  0.0023,  0.0006,  ...,  0.0122,  0.0104, -0.0184],\n",
      "        [-0.0119,  0.0300,  0.0294,  ...,  0.0075, -0.0299,  0.0283],\n",
      "        [-0.0085,  0.0290,  0.0280,  ...,  0.0209, -0.0267,  0.0157],\n",
      "        [-0.0045, -0.0095, -0.0263,  ..., -0.0310, -0.0134, -0.0171]],\n",
      "       requires_grad=True)\n",
      "FC4.bias : Parameter containing:\n",
      "tensor([ 0.0085, -0.0110,  0.0213,  0.0013], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in g.named_parameters():\n",
    "    print(name,':',parameters)\n",
    "#     print(parameters)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 查看网络参数\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Step4. 分别定义两个网络的损失函数和优化方法，进行训练\n",
    "'''\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "# 这里产生一个固定的噪声，每轮学习完成后都使用固定噪声来进行输出\n",
    "# fixed_noise = torch.randn(25, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "optimizerD = torch.optim.Adam(NetD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(NetG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Step5. 开始进行网络训练\n",
    "首先对鉴别器进行训练，然后训练生成器\n",
    "'''\n",
    "# Training Loop\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "dataset = NumbersDataset(0, 50)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        NetD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = NetD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = NetG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = NetD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        NetG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = NetD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}